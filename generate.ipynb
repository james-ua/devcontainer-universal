{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in ./.venv/lib/python3.11/site-packages (24.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/codespace/.local/lib/python3.11/site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.11/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 11:12:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/workspaces/devcontainer-universal/spark-warehouse')]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F, types as t\n",
    "from pyspark.sql.window import Window\n",
    "import random\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"pyspark-test01\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "print(spark.catalog.listDatabases())\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "UDFs"
    ]
   },
   "outputs": [],
   "source": [
    "@F.udf(t.IntegerType())\n",
    "def f_int(min:int, max:int) -> int:\n",
    "    return fake.random_int(min=min, max=max)\n",
    "\n",
    "@F.udf\n",
    "def f_intUnique(digits:int) -> int:\n",
    "    return fake.unique.random_number(digits=digits)\n",
    "\n",
    "@F.udf\n",
    "def f_choise(list:list, weights:list) -> str:\n",
    "    return random.choices(list,weights=weights)[0]\n",
    "\n",
    "@F.udf\n",
    "def f_bool() -> bool:\n",
    "    return fake.boolean()\n",
    "\n",
    "@F.udf\n",
    "def f_GUID() -> str:\n",
    "    return fake.uuid4()\n",
    "\n",
    "@F.udf\n",
    "def f_hash() -> str:\n",
    "    return fake.md5(raw_output=False)\n",
    "\n",
    "@F.udf\n",
    "def f_date(start:str='-3y', end:str='now') -> date:\n",
    "    return fake.date_between(start_date=start, end_date=end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|Consumer_Key|random_int|\n",
      "+------------+----------+\n",
      "|        1701|         1|\n",
      "|       23735|         1|\n",
      "|       35873|         2|\n",
      "|       43076|         1|\n",
      "|       43219|         1|\n",
      "|       48349|         1|\n",
      "|       50710|         1|\n",
      "|       52774|         1|\n",
      "|       71603|         2|\n",
      "|       79415|         1|\n",
      "|       79470|         1|\n",
      "|       81071|         1|\n",
      "|      101757|         1|\n",
      "|      103119|         1|\n",
      "|      109751|         1|\n",
      "|      120461|         1|\n",
      "|      127543|         2|\n",
      "|      131637|         1|\n",
      "|      133033|         2|\n",
      "|      139259|         2|\n",
      "+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consumers = spark.read.table('consumer_dimension').select('Consumer_Key')\n",
    "consumers.count()\n",
    "\n",
    "tmp_df = consumers.withColumn('random_int', f_int(F.lit(1),F.lit(2)))\n",
    "tmp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupBy_columns = ['Division_Code_Key','Region_Code_Key','Affiliate_Code_Key','Market_Code_Key']\n",
    "\n",
    "# Total Buyers, will join it to Consumer Dimention on Consumer Master Reg GUID\n",
    "trans_head = spark.read.table('transaction_header').select('Source_Consumer_Key')\n",
    "master_reg = spark.read.table('master_registry').select('Source_Consumer_Key', 'Consumer_Master_Registry_Global_Unique_Identifier')\n",
    "total_buyers = (\n",
    "    master_reg\n",
    "    .join(trans_head, master_reg.Source_Consumer_Key == trans_head.Source_Consumer_Key, 'left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lifecycle\n",
    "lifecycle = (\n",
    "    spark.read.table('consumer_lifecycle')\n",
    "    .select('Consumer_Key','Consumer_Lifecycle_Status_Code_Key')\n",
    "    .where(F.col('Consumer_Lifecycle_Status_Current_Record_Indicator') == 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[INSERT_COLUMN_ARITY_MISMATCH.TOO_MANY_DATA_COLUMNS] Cannot write to `spark_catalog`.`default`.`consumer_address`, the reason is too many data columns:\nTable columns: `Consumer_Address_Key`, `Consumer_Full_Address`, `Consumer_Address_Country_Code_Key`, `Consumer_Address_Last_Update_Source_System_Code_Key`, `Consumer_Address_Last_Postal_Opt_In_Opt_Out_Update_Source_System_Code_Key`, `Consumer_Key`, `Consumer_Address_Type_Code_Key`, `Consumer_Address_Record_Valid_From_Timestamp`, `Consumer_Address_Record_Valid_To_Timestamp`, `Consumer_Address_Current_Record_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Update_Timestamp`, `Consumer_Address_Source_Last_Update_Timestamp`, `Consumer_Line1_Address`, `Consumer_Line2_Address`, `Consumer_Line3_Address`, `Consumer_Address_City_Name`, `Consumer_Address_Zip_Code`, `Consumer_Address_Melissa_Identifier`, `Consumer_Address_Melissa_Delivery_Type_Code`, `Consumer_Address_Melissa_Result_Code`, `Consumer_Address_Melissa_Quality_Score_Code`, `Consumer_Address_Record_Created_Timestamp`, `Consumer_Address_Record_Modified_Timestamp`.\nData columns: `Consumer_Address_Key`, `Consumer_Address_Country_Code_Key`, `Consumer_Address_Last_Update_Source_System_Key`, `Consumer_Address_Last_Postal_Opt_In_Opt_Out_Update_Source_System_Key`, `Consumer_Key`, `Consumer_Address_Type_Code_Key`, `Consumer_Address_Record_Valid_From_Date`, `Consumer_Address_Record_Valid_To_Date`, `Consumer_Full_Address`, `Consumer_Address_Current_Record_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Update_Date`, `Consumer_Address_Source_Last_Update_Date`, `Consumer_Line1_Address`, `Consumer_Line2_Address`, `Consumer_Line3_Address`, `Consumer_Address_City_Name`, `Consumer_Address_Zip_Code`, `Consumer_Address_Melissa_Identifier`, `Consumer_Address_Melissa_Delivery_Type_Code`, `Consumer_Address_Melissa_Result_Code`, `Consumer_Address_Melissa_Quality_Score_Code`, `Consumer_Address_Record_Created_Timestamp`, `Consumer_Address_Record_Modified_Timestamp`, `Consumer_Address_Last_Update_Source_System_Code_Key`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_consumer_address \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/address_2024_03_29.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumer_Address_Key\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumer_Address_Key\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigint\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumer_Address_Last_Update_Source_System_Code_Key\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumer_Address_Last_Update_Source_System_Key\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigint\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcsv_consumer_address\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsertInto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconsumer_address\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/devcontainer-universal/.venv/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1513\u001b[0m, in \u001b[0;36mDataFrameWriter.insertInto\u001b[0;34m(self, tableName, overwrite)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1513\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsertInto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/devcontainer-universal/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/workspaces/devcontainer-universal/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INSERT_COLUMN_ARITY_MISMATCH.TOO_MANY_DATA_COLUMNS] Cannot write to `spark_catalog`.`default`.`consumer_address`, the reason is too many data columns:\nTable columns: `Consumer_Address_Key`, `Consumer_Full_Address`, `Consumer_Address_Country_Code_Key`, `Consumer_Address_Last_Update_Source_System_Code_Key`, `Consumer_Address_Last_Postal_Opt_In_Opt_Out_Update_Source_System_Code_Key`, `Consumer_Key`, `Consumer_Address_Type_Code_Key`, `Consumer_Address_Record_Valid_From_Timestamp`, `Consumer_Address_Record_Valid_To_Timestamp`, `Consumer_Address_Current_Record_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Update_Timestamp`, `Consumer_Address_Source_Last_Update_Timestamp`, `Consumer_Line1_Address`, `Consumer_Line2_Address`, `Consumer_Line3_Address`, `Consumer_Address_City_Name`, `Consumer_Address_Zip_Code`, `Consumer_Address_Melissa_Identifier`, `Consumer_Address_Melissa_Delivery_Type_Code`, `Consumer_Address_Melissa_Result_Code`, `Consumer_Address_Melissa_Quality_Score_Code`, `Consumer_Address_Record_Created_Timestamp`, `Consumer_Address_Record_Modified_Timestamp`.\nData columns: `Consumer_Address_Key`, `Consumer_Address_Country_Code_Key`, `Consumer_Address_Last_Update_Source_System_Key`, `Consumer_Address_Last_Postal_Opt_In_Opt_Out_Update_Source_System_Key`, `Consumer_Key`, `Consumer_Address_Type_Code_Key`, `Consumer_Address_Record_Valid_From_Date`, `Consumer_Address_Record_Valid_To_Date`, `Consumer_Full_Address`, `Consumer_Address_Current_Record_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Indicator`, `Consumer_Address_Postal_Opt_In_Opt_Out_Update_Date`, `Consumer_Address_Source_Last_Update_Date`, `Consumer_Line1_Address`, `Consumer_Line2_Address`, `Consumer_Line3_Address`, `Consumer_Address_City_Name`, `Consumer_Address_Zip_Code`, `Consumer_Address_Melissa_Identifier`, `Consumer_Address_Melissa_Delivery_Type_Code`, `Consumer_Address_Melissa_Result_Code`, `Consumer_Address_Melissa_Quality_Score_Code`, `Consumer_Address_Record_Created_Timestamp`, `Consumer_Address_Record_Modified_Timestamp`, `Consumer_Address_Last_Update_Source_System_Code_Key`."
     ]
    }
   ],
   "source": [
    "csv_consumer_address = (\n",
    "    spark.read.csv('src/address_2024_03_29.csv', header=True, inferSchema=True)\n",
    "    .withColumn('Consumer_Address_Key', F.col('Consumer_Address_Key').cast('bigint'))\n",
    "    .withColumn('Consumer_Address_Last_Update_Source_System_Code_Key', F.col('Consumer_Address_Last_Update_Source_System_Key').cast('bigint'))\n",
    ")\n",
    "csv_consumer_address.write.insertInto('consumer_address')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loyalty\n",
    "loyalty = spark.read.table('consumer_loyalty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+---------------+------------------+---------------+-------------------------------------------+---------------------------------------+------------------------------------------------+-----------------------------------------+\n",
      "|Consumer_Key|Division_Code_Key|Region_Code_Key|Affiliate_Code_Key|Market_Code_Key|Consumer_Phone_SMS_Opt_In_Opt_Out_Indicator|Consumer_Email_Opt_In_Opt_Out_Indicator|Consumer_Address_Postal_Opt_In_Opt_Out_Indicator|Consumer_Address_Current_Record_Indicator|\n",
      "+------------+-----------------+---------------+------------------+---------------+-------------------------------------------+---------------------------------------+------------------------------------------------+-----------------------------------------+\n",
      "|        1701|                1|             10|            886963|              1|                                          0|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       23735|                3|             10|            184991|              1|                                          1|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       35873|               10|             10|            588046|              1|                                       NULL|                                      0|                                            NULL|                                     NULL|\n",
      "|       43076|               12|             10|            800039|              1|                                       NULL|                                      1|                                            NULL|                                     NULL|\n",
      "|       43219|               22|             10|            733440|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       48349|               21|             10|            449523|              1|                                          1|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       50710|               17|             10|            516335|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       52774|               12|             10|            754082|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       71603|               21|             10|            260413|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       79415|               10|             10|            586846|              1|                                          0|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       79470|                1|             10|            809587|              1|                                          1|                                   NULL|                                            NULL|                                     NULL|\n",
      "|       81071|               21|             10|            317140|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|      101757|               12|             10|            989387|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|      103119|               26|             10|            212090|              1|                                          1|                                   NULL|                                            NULL|                                     NULL|\n",
      "|      109751|               17|             10|            319009|              1|                                       NULL|                                      0|                                            NULL|                                     NULL|\n",
      "|      120461|               11|             10|            483619|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|      127543|               26|             10|            123148|              1|                                          1|                                   NULL|                                            NULL|                                     NULL|\n",
      "|      131637|               10|             10|            772218|              1|                                       NULL|                                      1|                                            NULL|                                     NULL|\n",
      "|      133033|               12|             10|            180857|              1|                                       NULL|                                   NULL|                                            NULL|                                     NULL|\n",
      "|      139259|               26|             10|            493745|              1|                                          1|                                   NULL|                                            NULL|                                     NULL|\n",
      "+------------+-----------------+---------------+------------------+---------------+-------------------------------------------+---------------------------------------+------------------------------------------------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contactables\n",
    "phone = spark.read.table('consumer_phone').select('Consumer_Key','Consumer_Phone_SMS_Opt_In_Opt_Out_Indicator').where(F.col('Consumer_Phone_Current_Record_Indicator') == 1)\n",
    "email = spark.read.table('consumer_email').select('Consumer_Key','Consumer_Email_Opt_In_Opt_Out_Indicator').where(F.col('Consumer_Email_Current_Record_Indicator') == 1)\n",
    "address = spark.read.table('consumer_address').select('Consumer_Key','Consumer_Address_Postal_Opt_In_Opt_Out_Indicator','Consumer_Address_Current_Record_Indicator').where(F.col('Consumer_Address_Current_Record_Indicator')==1)\n",
    "consumers = spark.read.table('consumer_dimension').select('Consumer_Key', *groupBy_columns)\n",
    "contactable = (consumers\n",
    "               .join(phone, consumers.Consumer_Key == phone.Consumer_Key, 'left').drop(phone.Consumer_Key)\n",
    "               .join(email, consumers.Consumer_Key == email.Consumer_Key, 'left').drop(email.Consumer_Key)\n",
    "               .join(address, consumers.Consumer_Key == address.Consumer_Key, 'left').drop(address.Consumer_Key)\n",
    ")\n",
    "contactable.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
